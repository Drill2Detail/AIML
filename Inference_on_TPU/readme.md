# Youtube Links:

- [Introduction: Architecting an AI inference stack (with GPUs or TPUs)](https://www.youtube.com/watch?v=iqow3JWUjbA)
- [AI/ML frameworks for cloud TPUs](https://www.youtube.com/watch?v=nguJYDAHOi4)
    - Nvidia Triton 
    - TGI ->  
    - vLLM
    - LLM-d
- [Model types and performance bottlenecks](https://www.youtube.com/watch?v=XmJUDAfxmWs)
    - Compute 
    - Memory 
    - Memory Bandwidth      
    - LLMs / Diffusion Models / Visual Language Models / Mixture of Experts 
- [AI workload orchestration options](https://www.youtube.com/watch?v=DpH3LuTnvsY)
    - Cluster Director
    - Ray 
    - Slurm 
- [Serving AI models at scale with vLLM](https://www.youtube.com/watch?v=kH63PGZsDY4)
- [Deploying scalable and reliable AI inference on Google Cloud](https://www.youtube.com/watch?v=hkXGXen6eL4)
    - Anywhere Cache 
    - Managed Lustre 
- [Run your AI inference application on TPUs](https://www.youtube.com/watch?v=p24jvCnU43Y)
- [Scaling AI with Google Cloud's TPUs](https://www.youtube.com/watch?v=8zBaa20mvoc)
- [Serving AI models at scale with vLLM](https://www.youtube.com/watch?v=kH63PGZsDY4)
- [vLLM demo](https://www.youtube.com/watch?v=XrHqe0pzYQw)
- [Google_Cloud_AI_Infrastructure](https://discuss.google.dev/c/google-cloud/cloud-build-ai/cloud-ai-infrastructure/209)

# Resources: 

- [Welcome to vLLM](https://goo.gle/49zlRZN)
- [TPU Inference GitHub](https://goo.gle/3JUkBpn)
- [AI-Inference-recipe-using-Nvidia-Dynamo](https://cloud.google.com/blog/products/compute/ai-inference-recipe-using-nvidia-dynamo-with-ai-hypercomputer)
- [Github AI-HyperComputer](https://github.com/AI-Hypercomputer/tpu-recipes/tree/main)


